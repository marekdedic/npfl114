### Lecture: 13. Transformer, BERT
#### Date: May 25
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/1920/slides/?13
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/1920/slides.pdf/npfl114-13.pdf,PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/1920/npfl114-13.mp4,Video
#### Questions: #lecture_13_questions
#### Lecture assignment: sentiment_analysis
#### VideoPlayer: npfl114-13.mp4,npfl114-13.practicals.mp4

- Transformer architecture [[Attention Is All You Need](https://arxiv.org/abs/1706.03762)]
- BERT [[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)]
- _RoBERTa [[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)]_
- _ALBERT [[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)]_
