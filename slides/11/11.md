title: NPFL114, Lecture 11
class: title, langtech, cc-by-nc-sa

# Introduction to Deep Reinforcement Learning

## Milan Straka

### May 11, 2020

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s – Richard Bellman

~~~
- Trial and error learning – since 1850s
  - Law and effect – Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, … – 1950s and 1960s
  - Tsetlin, Holland, Klopf – 1970s
  - Sutton, Barto – since 1980s

~~~
- Arthur Samuel – first implementation of temporal difference methods
  for playing checkers

~~~
- Gerry Tesauro – 1992, human-level Backgammon playing program trained solely by self-play
---
# Notable Successes of Reinforcement Learning

- IBM Watson in Jeopardy – 2011

~~~
- Human-level video game playing (DQN) – 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C – 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow – 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala – Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala – Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# Notable Successes of Reinforcement Learning

![w=22%,f=right](r2d2-results.pdf)

- R2D2 – Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training

~~~
- MuZero – Nov 2019
  - planning with a learned model: 4999.2%, median: 2041.1%

~~~
- Data-efficient Rainbow – Jun 2019
  - learning from ~2 hours of game experience
![w=34%,mw=70%,h=center](der-progress.pdf)

---
# Notable Successes of Reinforcement Learning
- AlphaGo

  - Mar 2016 – beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master – Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero – 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero – Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=25%,h=center](a0_results.pdf)

---
# Notable Successes of Reinforcement Learning

- Dota2 – Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN – Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW – Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)

~~~
- OpenAI Five – Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs; 180 years of experience per day

~~~
- AlphaStar – 2019
  - Jan: won 10 out of 11 StarCraft II games against two professional players
  - Oct: in the full game of StarCraft II, got above 99.8% of oficially
    ranked players

---
# Notable Successes of Reinforcement Learning

- Neural Architecture Search – since 2017

  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
~~~
  - AutoML: automatically discovering
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - …
~~~
- System for automatic control of data-center cooling – 2017

~~~
- AlphaFold won CASP13, a global competition in protein structure prediction

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, …

~~~
Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = 𝔼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ≝ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t ≝ \argmax_a Q_t(a).$$

---
section: $ε$-greedy
# $ε$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$ε$-greedy_ method follows the greedy action with probability $1-ε$, and
chooses a uniformly random action with probability $ε$.

---
# $ε$-greedy Method

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# $ε$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} ∑_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ∑_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $ε$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.pdf)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
~~~
- $𝓐$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a _reward_ $r ∈ ℝ$,
~~~
- $γ ∈ [0, 1]$ is a _discount factor_ (we will always use $γ=1$ and finite episodes in this course).

~~~
Let a _return_ $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $𝓢=\{S\}$;
~~~
- an action for every arm, $𝓐=\{a_1, a_2, …, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $𝓝(μ_i, σ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = 𝓝(r | μ_i, σ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to have
separate reward distribution for every state. Such generalization is
usually called _Contextualized Bandits_ problem.
Assuming that state transitions are independent on rewards and given by
a distribution $\textit{next}(s)$, the MDP dynamics function for contextualized
bandits problem is given by
$$p(s', r | s, a_i) = 𝓝(r | μ_{i,s}, σ_{i,s}^2) ⋅ \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called _episodes_, we talk about **episodic tasks**.

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These _finite-horizont tasks_ then can use discount factor $γ=1$,
because the return $G ≝ ∑_{t=0}^H γ^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $γ$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A _policy_ $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_π(s)$, or
_state-value function_, as
$$v_π(s) ≝ 𝔼_π\left[G_t \middle| S_t = s\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\left[G_t \middle| S_t = s, A_t = a\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_π(s) &= 𝔼_π[q_π(s, a)], \\
  q_π(s, a) &= 𝔼_π[R_{t+1} + γv_π(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ≝ \max_π v_π(s),$$
~~~
analogously
$$q_*(s, a) ≝ \max_π q_π(s, a).$$

~~~
Any policy $π_*$ with $v_{π_*} = v_*$ is called an _optimal policy_. Such policy
can be defined as $π_*(s) ≝ \argmax_a q_*(s, a) = \argmax_a 𝔼[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizont tasks or if $γ < 1$, there always exists a unique optimal
state-value function, unique optimal action-value function, and (not necessarily
unique) optimal policy.

---
section: MonteCarlo
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $𝓢$, finitely many
actions $𝓐$ and we will store estimates for every possible state-action pair.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

---
# Monte Carlo and $ε$-soft Policies

For the estimates to converge to the real values, every reachable state must be
visited infinitely many times and every action in such a state must be selected
infinitely many times in limit.

~~~
A policy is called $ε$-soft, if
$$π(a|s) ≥ \frac{ε}{|𝓐(s)|}.$$

~~~
We call a policy $ε$-greedy, if one action has maximum probability of
$1-ε+\frac{ε}{|A(s)|}$.

~~~
It can be shown that when considering the class of $ε$-soft policies, one of the
optimal policies is always $ε$-greedy – we will therefore search among the
$ε$-greedy policies only.

---
# Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{t+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: REINFORCE
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Theorem

Assume that $𝓢$ and $𝓐$ are finite and that maximum episode length $H$ is also finite.

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.

~~~
Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is probability of being in state $s'$ when starting
from state $s$, after any number of 0, 1, … steps.


---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \qquad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \Big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} ∑_{k=0}^H P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, it is enough to realize that
$$∑\nolimits_{k=0}^H P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∝ P(s → … → s'|π).$$

~~~
For the second part, we know that
$$∇_→θ J(→θ) = 𝔼_{s ∼ h} ∇_→θ v_π(s) ∝ 𝔼_{s ∼ h} ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ),$$
~~~
therefore using the fact that $μ(s') = 𝔼_{s ∼ h} P(s → … → s'|π)$ we get
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

~~~
Finally, note that the theorem can be proved with infinite $𝓢$ and $𝓐$; and
also for infinite episodes when discount factor $γ<1$.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(→θ) ≝ -𝔼_{h, π} v_π(s)$. The loss gradient is then
$$∇_→θ -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_→θ -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_→θ \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.pdf)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called an _advantage function_
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.pdf)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.pdf)
