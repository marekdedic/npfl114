title: NPFL114, Lecture 12
class: title, langtech, cc-by-nc-sa

# NASNet, Speech Synthesis,<br>External Memory Networks

## Milan Straka

### May 18, 2020

---
section: NASNet
# Neural Architecture Search (NASNet) – 2017

- We can design neural network architectures using reinforcement learning.

~~~
- The designed network is encoded as a sequence of elements, and is generated
  using an _RNN controller_, which is trained using the REINFORCE with baseline
  algorithm.

![w=55%,h=center](nasnet_overview.pdf)

~~~
- For every generated sequence, the corresponding network is trained on CIFAR-10
  and the development accuracy is used as a return.

---
# Neural Architecture Search (NASNet) – 2017

The overall architecture of the designed network is fixed and only the Normal
Cells and Reduction Cells are generated by the controller.

![w=29%,h=center](nasnet_overall.pdf)

---
# Neural Architecture Search (NASNet) – 2017

- Each cell is composed of $B$ blocks ($B=5$ is used in NASNet).
~~~
- Each block is designed by a RNN controller generating 5 parameters.

![w=80%,h=center](nasnet_rnn_controller.pdf)

![w=60%,mw=50%,h=center](nasnet_block_steps.pdf)![w=80%,mw=50%,h=center](nasnet_operations.pdf)

- Every block is designed by a RNN controller generating individual operations.

---
# Neural Architecture Search (NASNet) – 2017

The final proposed Normal Cell and Reduction Cell:

![w=77%,h=center](nasnet_blocks.pdf)

---
# EfficientNet Search

EfficientNet changes the search in two ways.

~~~
- Computational requirements are part of the return. Notably, the goal is to
  find an architecture $m$ maximizing
  $$\operatorname{DevelopmentAccuracy}(m) ⋅ \left(\frac{\textrm{TargetFLOPS=400M}}{\operatorname{FLOPS}(m)}\right)^{0.07}$$
  where the constant $0.07$ balances the accuracy and FLOPS.

~~~
- Using a different search space, which allows to control kernel sizes and
  channels in different parts of the overall architecture (compared to using the
  same cell everywhere as in NASNet).

---
# EfficientNet Search

![w=100%](mnasnet_overall.pdf)

![w=30%,f=right](mnasnet_parameters.pdf)

The overall architecture consists of 7 blocks, each described by 6 parameters
– 42 parameters in total, compared to 50 parameters of NASNet search space.

---
# EfficientNet-B0 Baseline Network

![w=100%](../05/efficientnet_architecture.pdf)

---
section: WaveNet
# WaveNet

Our goal is to model speech, using a auto-regressive model
$$P(→x) = ∏_t P(x_t | x_{t-1}, …, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.pdf)

---
# WaveNet

![w=100%,v=middle](wavenet_dilated_convolutions.pdf)

---
# WaveNet

## Output Distribution
The raw audio is usually stored in 16-bit samples. However, classification
into $65\,536$ classes would not be tractable, and instead WaveNet adopts
$μ$-law transformation and quantize the samples into 256 values using
$$\operatorname{sign}(x)\frac{\ln(1 + 255|x|)}{\ln(1 + 255)}.$$

~~~
## Gated Activation
To allow greater flexibility, the outputs of the dilated convolutions are passed
through the gated activation units
$$→z = \tanh(⇉W_f * →x) ⋅ σ(⇉W_g * →x).$$

---
# WaveNet

![w=80%,h=center](wavenet_block.pdf)

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $→h$,
changing the gated activation function to
$$→z = \tanh(⇉W_f * →x + ⇉V_f→h) ⋅ σ(⇉W_g * →x + ⇉V_g→h).$$

~~~
## Local Conditioning
For local conditioning, we are given a time series $h_t$, possibly with a lower
sampling frequency. We first use transposed convolutions $→y = f(→h)$ to match resolution
and then compute analogously to global conditioning
$$→z = \tanh(⇉W_f * →x + ⇉V_f * →y) ⋅ σ(⇉W_g * →x + ⇉V_g * →y).$$

---
# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 3
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1×1$ output convolution produces 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of $0.0002$

---
# WaveNet

![w=85%,h=center](wavenet_results.pdf)

---
section: ParallelWaveNet
# Parallel WaveNet

The output distribution was changed from 256 $μ$-law values to a Mixture of
Logistic (suggested in another paper – PixelCNN++, but reused in other architectures since):
$$ν ∼ ∑_i π_i \operatorname{logistic}(μ_i, s_i).$$

~~~
The logistic distribution is a distribution with a $σ$ as cumulative density function
(where the mean and scale is parametrized by $μ$ and $s$). Therefore, we can
write
$$P(x | →π, →μ, →s) = ∑_i π_i \big[σ((x + 0.5 - μ_i) / s_i) - σ((x - 0.5 - μ_i) / s_i)\big],$$
where we replace -0.5 and 0.5 in the edge cases by $-∞$ and $∞$.

~~~
In Parallel WaveNet, 10 mixture components are used.

---
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we use a following trick. We will model $P(x_t)$ as $P(x_t | →z_{<t})$
for a _random_ $→z$ drawn from a logistic distribution. Then, we compute
$$x^1_t = z_t ⋅ s^1(→z_{< t}) + μ^1(→z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
– 4 iterations were used by the authors. In further iterations,
$$x^i_t = x^{i-1}_t ⋅ s^i(→x^{i-1}_{< t}) + μ^i(→x^{i-1}_{< t}).$$

~~~
After $N$ iterations, $P(→x^N_t | →z_{≤t})$ is a logistic distribution
with location $→μ_\textrm{tot}$ and scale $→s_\textrm{tot}$ with
$$→μ_\textrm{tot} = ∑_i^N μ^i(→x^{i-1}_{< t}) \cdot \left(∏\nolimits_{j>i}^N s^j(→x^{j-1}_{< t})\right)
\textrm{~~and~~}
→s_\textrm{tot} = ∑_i^N s^i(→x^{i-1}_{< t}).$$


---
# Parallel WaveNet

The network is trained using a _probability density distillation_ using
a teacher WaveNet, using KL-divergence as loss.

![w=75%,h=center](parallel_wavenet_distillation.pdf)

---
# Parallel WaveNet

Denoting the teacher distribution as $P_T$ and the student distribution
as $P_S$, the loss is specifically
$$D_\textrm{KL}(P_S || P_T) = H(P_S, P_T) - H(P_S).$$

~~~
Therefore, we do not only minimize cross-entropy, but we also try to keep the
entropy of the student as high as possible. That is crucial not to match
just the mode of the teacher. (Consider a teacher generating white noise,
where every sample comes from $𝓝(0, 1)$ – in this case, the cross-entropy
loss of a constant $→0$, complete silence, would be maximal.)

~~~
In a sense, probability density distillation is similar to GANs. However,
the teacher is kept fixed and the student does not attempt to fool it
but to match its distribution instead.

---
# Parallel WaveNet

With the 4 iterations, the Parallel WaveNet generates over 500k samples per
second, compared to ~170 samples per second of a regular WaveNet – more than
a 1000 times speedup.

![w=80%,mh=80%,v=bottom,h=center](parallel_wavenet_results.pdf)

---
section: Tacotron
# Tacotron

![w=65%,h=center](tacotron.pdf)

---
# Tacotron

![w=65%,h=center,v=middle](tacotron_results.pdf)

---
# Tacotron

![w=100%,v=middle](tacotron_comparison.pdf)

---
section: NTM
# Neural Turing Machines

So far, all input information was stored either directly in network weights, or
in a state of a recurrent network.

~~~
However, mammal brains seem to operate with a _working memory_ – a capacity for
short-term storage of information and its rule-based manipulation.

~~~
We can therefore try to introduce an external memory to a neural network. The
memory $⇉M$ will be a matrix, where rows correspond to memory cells.

---
# Neural Turing Machines

The network will control the memory using a controller which reads from the
memory and writes to is. Although the original paper also considered
a feed-forward (non-recurrent) controller, usually the controller is a recurrent
LSTM network.

![w=55%,h=center](ntm_architecture.pdf)

---
# Neural Turing Machine

## Reading

To read the memory in a differentiable way, the controller at time $t$ emits
a read distribution $→w_t$ over memory locations, and the returned read vector $→r_t$
is then
$$→r_t = ∑_i w_t(i) ⋅ →M_t(i).$$

## Writing

Writing is performed in two steps – an _erase_ followed by an _add_. The
controller at time $t$ emits a write distribution $→w_t$ over memory locations,
and also an _erase vector_ $→e_t$ and an _add vector_ $→a_t$. The memory is then
updates as
$$→M_t(i) = →M_{t-1}(i)\big[1 - w_t(i)→e_t] + w_t(i) →a_t.$$

---
# Neural Turing Machine

The addressing mechanism is designed to allow both
- content addressing, and
- location addressing.

![w=90%,h=center](ntm_addressing.pdf)

---
# Neural Turing Machine

## Content Addressing

Content addressing starts by the controller emitting the _key vector_ $→k_t$,
which is compared to all memory locations $→M_t(i)$, generating a distribution
using a $\softmax$ with temperature $β_t$.
$$w_t^c(i) = \frac{\exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(i))}{∑_j \exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(j))}$$

The $\operatorname{distance}$ measure is usually the cosine similarity
$$\operatorname{distance}(→a, →b) = \frac{→a ⋅ →b}{||→a|| ⋅ ||→b||}.$$

---
# Neural Turing Machine

## Location-Based Addressing

To allow iterative access to memory, the controller might decide to reuse the
memory location from the previous timestep. Specifically, the controller emits
_interpolation gate_ $g_t$ and defines
$$→w_t^g = g_t →w_t^c + (1 - g_t) →w_{t-1}.$$

Then, the current weighting may be shifted, i.e., the controller might decide to
“rotate” the weights by a small integer. For a given range (the simplest case
are only shifts $\{-1, 0, 1\}$), the network emits $\softmax$ distribution over
the shifts, and the weights are then defined using a circular convolution
$$w̃_t(i) = ∑_j w_t^g(j) s_t(i - j).$$

Finally, not to lose precision over time, the controller emits
a _sharpening factor_ $γ_t$ and the final memory location weights are
$w_t(i) = {w̃_t(i)^{γ_t}} / {∑_j w̃_t(j)^{γ_t}}.$

---
# Neural Turing Machine

## Overall Execution

Even if not specified in the original paper, following the DNC paper, the LSTM
controller can be implemented as a (potentially deep) LSTM. Assuming $R$ read
heads and one write head, the input is $→x_t$ and $R$ read
vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from the previous time step, the output of the
controller are vectors $(→ν_t, →ξ_t)$, and the final output is
$→y_t = →ν_t + W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is a concatenation of
$$→k_t^1, β_t^1, g_t^1, →s_t^1, γ_t^1, →k_t^2, β_t^2, g_t^2, →s_t^2, γ_t^2, …, →k_t^w, β_t^w, g_t^w, →s_t^w, γ_t^w, →e_t^w, →a_t^w.$$

---
# Neural Turing Machines

## Copy Task

Repeat the same sequence as given on input. Trained with sequences of length up
to 20.

![w=70%,h=center](ntm_copy_training.pdf)

---
# Neural Turing Machines

![w=84%,h=center](ntm_copy_generalization.pdf)

---
# Neural Turing Machines

![w=95%,h=center](ntm_copy_generalization_lstm.pdf)

---
# Neural Turing Machines

![w=65%,h=center](ntm_copy_memory.pdf)

---
# Neural Turing Machines

## Associative Recall

In associative recall, a sequence is given on input, consisting of subsequences
of length 3. Then a randomly chosen subsequence is presented on input and the
goal is to produce the following subsequence.

![w=65%,h=center](ntm_associative_recall_training.pdf)

---
# Neural Turing Machines

![w=83%,h=center](ntm_associative_recall_generalization.pdf)

---
# Neural Turing Machines

![w=53%,h=center](ntm_associative_recall_memory.pdf)

---
section: DNC
# Differentiable Neural Computer

NTM was later extended to a Differentiable Neural Computer.

![w=82%,h=center](dnc_architecture.pdf)

---
# Differentiable Neural Computer

The DNC contains multiple read heads and one write head.

~~~
The controller is a deep LSTM network, with input at time $t$ being the current
input $→x_t$ and $R$ read vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from previous time
step. The output of the controller are vectors $(→ν_t, →ξ_t)$, and the final
output is $→y_t = →ν_t + W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is
a concatenation of parameters for read and write heads (keys, gates, sharpening
parameters, …).

~~~
In DNC, the usage of every memory location is tracked, which enables performing
dynamic allocation – at each time step, a cell with least usage can be allocated.

~~~
Furthermore, for every memory location, we track which memory location
was written to previously ($→b_t$) and subsequently ($→f_t$), allowing
to recover sequences in the order in which it was written, independently on the
real indices used.

~~~
The write weighting is defined as a weighted combination of the allocation
weighting and write content weighting, and read weighting is computed as a weighted
combination of read content weighting, previous write weighting, and subsequent
write weighting.


---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks.pdf)

---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks_traversal.pdf)

---
section: MANN
# Memory-augmented Neural Networks

External memory can be also utilized for _learning to learn_. Consider a network,
which should learn classification into a user-defined hierarchy by observing
ideally a small number of samples.

~~~
Apart from finetuning the model and storing the information in the _weights_,
an alternative is to store the samples in _external memory_. Therefore, the
model learns how to store the data and access it efficiently, which allows
it to learn without changing its weights.

![w=100%](mann_overview.pdf)

---
# Memory-augmented NNs

![w=90%,mw=62%](mann_reading.pdf)![w=38%](mann_writing.pdf)

---
# Memory-augmented NNs

![w=60%,h=center](mann_results.pdf)
