title: NPFL114, Lecture 13
class: title, langtech, cc-by-nc-sa

# Transformer, BERT

## Milan Straka

### May 25, 2020

---
section: Transformer
# Attention is All You Need

For some sequence processing tasks, _sequential_ processing (as performed by
recurrent neural networks) of its elements might be too restrictive.

~~~
Instead, we may want to be able to combine sequence elements independently on
their distance.

~~~
Such processing is allowed in the *Transformer* architecture, originally
proposed for neural machine translation in 2017 in *Attention is All You Need*
paper.

---
# Transformer

![w=33%,h=center](transformer.pdf)

---
# Transformer

![w=77%,h=center](illustrated_transformer_encoder_decoder.png)

---
# Transformer

![w=100%,v=middle](illustrated_transformer_layer.png)

---
# Transformer

![w=82%,h=center](illustrated_transformer_encoder.png)

---
section: SelfAttention
# Transformer – Self-Attention

Assume that we have a sequence of $n$ words represented using a matrix $⇉X ∈ ℝ^{n×d}$.

The attention module for a queries $⇉Q ∈ ℝ^{n×d_k}$, keys $⇉K ∈ ℝ^{n×d_k}$ and values $⇉V ∈ ℝ^{n×d_v}$ is defined as:

$$\textrm{Attention}(⇉Q, ⇉K, ⇉V) = \softmax\left(\frac{⇉Q ⇉K^\top}{\sqrt{d_k}}\right)⇉V.$$

The queries, keys and values are computed from the input word representations $⇉X$
using a linear transformation as
$$\begin{aligned}
  ⇉Q &= ⇉W^Q ⋅ ⇉X \\
  ⇉K &= ⇉W^K ⋅ ⇉X \\
  ⇉V &= ⇉W^V ⋅ ⇉X \\
\end{aligned}$$

---
# Transformer – Self-Attention

![w=80%,h=center](illustrated_transformer_self_attention_inputs.png)

---
# Transformer – Self-Attention

![w=53%,h=center](illustrated_transformer_self_attention_outputs.png)

---
# Transformer – Self-Attention

![w=100%,v=middle](illustrated_transformer_self_attention_example.png)

---
# Transformer – Self-Attention

![w=40%](illustrated_transformer_self_attention_calculation_1.png)![w=60%](illustrated_transformer_self_attention_calculation_2.png)

---
# Transformer – Multihead Attention

Multihead attention is used in practice. Instead of using one huge attention, we
split queries, keys and values to several groups (similar to how ResNeXt works),
compute the attention in each of the groups separately, and then concatenate the
results.

![w=75%,h=center](transformer_multihead.pdf)

---
# Transformer – Multihead Attention

![w=85%,h=center](illustrated_transformer_attention_heads_input.png)

---
# Transformer – Multihead Attention

![w=42%,v=middle](illustrated_transformer_attention_heads_output.png)![w=58%](illustrated_transformer_attention_heads_output_2.png)

---
# Transformer – Multihead Attention

![w=90%,h=center](illustrated_transformer_multihead_attention.png)

---
# Why Attention

![w=100%,v=middle](transformer_attentions.pdf)

---
# Transformer – Feed Forward Networks

## Feed Forward Networks

The self-attention is complemented with FFN layers, which is a fully connected
ReLU layer with four times as many hidden units as inputs, followed by another
fully connected layer without activation.

![w=50%,h=center](../07/layer_norm_residual.pdf)

---
# Transformer – Residuals

![w=53%,h=center](illustrated_transformer_encoder_residual.png)

---
# Transformer – Decoder

![w=72%,v=middle](illustrated_transformer_decoder.png)![w=28%](transformer.pdf)

---
# Transformer – Decoder

![w=28%,f=right](transformer.pdf)

## Masked Self-Attention

During decoding, the self-attention must attent only to earlier positions in the
output sequence.

~~~
This is achieved by _masking_ future positions, i.e., zeroing their weights out,
which is usually implemented by setting them to $-∞$ before the $\softmax$ calculation.

~~~

## Encoder-Decoder Attention

In the encoder-decoder attentions, the _queries_ comes from the decoder, while the
_keys_ and the _values_ originate from the encoder.

---
section: PositionalEmbedding
# Transformer – Positional Embedding

![w=91%,h=center](illustrated_transformer_positional_encoding.png)

---
# Transformer – Positional Embeddings

## Positional Embeddings

We need to encode positional information (which was implicit in RNNs).

~~~
- Learned embeddings for every position.

~~~
- Sinusoids of different frequencies:
  $$\small\begin{aligned}
    \textrm{PE}_{(\textit{pos}, 2i)} & = \sin\left(\textit{pos} / 10000^{2i/d}\right) \\
    \textrm{PE}_{(\textit{pos}, 2i + 1)} & = \cos\left(\textit{pos} / 10000^{2i/d}\right)
  \end{aligned}$$

~~~
  This choice of functions should allow the model to attend to relative
  positions, since for any fixed $k$, $\textrm{PE}_{\textit{pos} + k}$ is
  a linear function of $\textrm{PE}_\textit{pos}$, because
  $$\small\begin{aligned}
    \textrm{PE}_{(\textit{pos}+k, 2i)}
      &= \sin\left((\textit{pos}+k) / 10000^{2i/d}\right) \\
      &= \sin\left(\textit{pos} / 10000^{2i/d}\right) ⋅ \cos\left(k / 10000^{2i/d}\right) + \cos\left(\textit{pos} / 10000^{2i/d}\right) ⋅ \sin\left(k / 10000^{2i/d}\right) \\
      &= \textit{offset}_{(k,2i)} ⋅ \textrm{PE}_{(\textit{pos}, 2i)} + \textit{offset}_{(k, 2i+1)} ⋅ \textrm{PE}_{(\textit{pos}, 2i + 1)}.
  \end{aligned}$$

---
# Transformer – Positional Embeddings

Positional embeddings for 20 words of dimension 512, lighter colors representing
values closer to 1 and darker colors representing values closer to -1.
![w=58%,h=center](transformer_positional_embeddings.png)

---
# Transformer – Training

## Regularization

The network is regularized by:
- dropout of input embeddings,
~~~
- dropout of each sub-layer, just before before it is added to the residual
  connection (and then normalized),
~~~
- label smoothing.

~~~
Default dropout rate and also label smoothing weight is 0.1.

~~~
## Parallel Execution
Because of the _masked attention_, training can be performed in parallel.

~~~
However, inference is still sequential.

---
# Transformer – Training

## Optimizer

Adam optimizer (with $β_2=0.98$, smaller than the default value of $0.999$)
is used during training, with the learning rate decreasing proportionally to
inverse square root of the step number.

~~~
## Warmup
Furthermore, during the first
$\textit{warmup\_steps}$ updates, the learning rate is increased linearly from
zero to its target value.

$$\textit{learning\_rate} = \frac{1}{\sqrt{d_\textrm{model}}} \min\left(\frac{1}{\sqrt{\textit{step\_num}}}, \frac{\textit{step\_num}}{\textit{warmup\_steps}} ⋅ \frac{1}{\sqrt{\textit{warmup\_steps}}}\right).$$

~~~
In the original paper, 4000 warmup steps were proposed.

---
# Transformers Results

![w=100%,v=middle](transformer_results.pdf)

---
# Transformers Results

![w=78%,h=center](transformer_ablations.pdf)

---
# Transformers Results

## Main Takeaway

Generally, Transformer provides more powerful sequence-to-sequence architecture
and also sequence element representation architecture than RNNs, but usually
requires substantially more data.

---
section: BERT
# ELMo

At the end of 2017, a new type of _deep contextualized_ word representations was
proposed by Peters et al., called ELMo, **E**mbeddings from **L**anguage
**Mo**dels.

~~~
The ELMo embeddings were based on a two-layer pre-trained LSTM language model,
where a language model predicts following word based on a sentence prefix.
Specifically, two such models were used, one for the forward direction and the
other one for the backward direction.

![w=30%](elmo_language_model.png)![w=68%](elmo_bidirectional.png)

---
# ELMo

To compute an embedding of a word in a sentence, the concatenation of the two
language model's hidden states is used.

![w=73%,h=center](elmo_embedding.png)

~~~
Pre-trained ELMo embeddings substantially improved several NLP tasks.

---
# BERT

A year later after ELMo, at the end of 2018, a new model called
BERT (standing for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers)
was proposed. It is nowadays one of the most dominating approaches for pre-training word
embeddings and for paragraph representation.

~~~
![w=100%](sesame_street.jpg)

---
# BERT

In the BERT model computes contextualized representations using a bidirectional
Transformer architecture.

![w=100%](bert_previous.pdf)

---
# BERT

The baseline BERT base model consists of 12 Transformer layers:
![w=80%,h=center](bert_encoders.png)

~~~
The bidirectionality is important, but it makes training difficult.

---
# BERT

![w=30%,f=right](bert_pretraining.png)

The input to the BERT model are two so-called _sentences_, but they are in fact
pieces of text with hundreds of subwords (512 maximum in total). The first token
is a special `CLS` token and every sentence is ended by a `SEP` token.
Additionally, a trainable embedding indicating if a token belongs to a sentence
`A` (inclusively up to its `SEP` token) or to sentence `B` is used.

~~~
The BERT model is pretrained using two objectives:
- **masked language model** – 15% of the input words are _masked_, and the model
  tries to predict them.

~~~
  - 80% of them are replaced by a special `MASK` token;
~~~
  - 10% of them are replaced by a random word;
~~~
  - 10% of them are left intact.

~~~
- **next sentence prediction** – the model tries to predict whether the second
  _sentence_ followed the first one in the raw corpus.

~~~
  - 50% of the time the second sentence is the actual next sentence;
~~~
  - 50% of the time the second sentence is a random sentence from the corpus.

---
# BERT

For pre-training, English BookCorpus (800M words) and Wikipedia (2,500M words)
are used, with a 30k WordPieces vocabulary.

~~~
Batch size is 256 sequences, each 512 subwords, giving 128k tokens per batch.
Adam with learning rate 1e-4 is used, with linear learning rate warmup for the
first 10k steps. Standard momentum parameters are used, and L2 weight decay of
0.01 is utilized.

~~~
Dropout of 0.1 on all layers is used, and GELU activation is used instead of
ReLU.

~~~
Furthermore, because longer sequences are quadratically more expensive, first
90% of the pre-training is performed on sequences of length 128, and only the
last 10% use sequences of length 512.

~~~
Two variants are considered:
- BERT _base_ with 12 layers, 12 attention heads and hidden size 768 (110M parameters),
~~~
- BERT _large_ with 24 layers, 16 attention heads and hidden size 1024 (340M parameters).

---
# BERT – GELU

ReLU multiplies the input by zero or one, depending on its value.

~~~
Dropout stochastically multiplies the input by zero or one.

~~~
![w=40%,f=right](gelu.pdf)

Both these functionalities are merged in Gaussian error linear units (GELUs),
where the input value is multiplied by $m ∼ \operatorname{Bernoulli}(Φ(x))$,
where $Φ(x) = P(x' ≤ x)$ for $x' ∼ 𝓝(0, 1)$ is the cumulative density function
of the standard normal distribution.

~~~
The GELUs compute the expectation of this value, i.e.,
$$\operatorname{GELU}(x) = x ⋅ Φ(x) + 0 ⋅ \big(1 - Φ(x)\big) = x Φ(x).$$

~~~
GELUs can be approximated using
$$0.5x \left(1 + \tanh\left[\sqrt{2/π}(x + 0.044715 x^3)\right]\right)\textrm{~~or~~}x σ(1.702x).$$

---
# BERT – Finetuning

![w=49%,f=right](bert_applications.pdf)

The pre-trained BERT model can be finetuned on a range of tasks:

- **sentence element representation**

  - PoS tagging
  - named entity recognition
  - …

~~~
- **sentence representation**

  - text classification

~~~
- **sentence relation representation**

  - textual entailment, aka natural language inference (the second sentence is
    _implied by/contradicts/has no relation to_ the first sentence)
  - textual similarity
  - paraphrase detection
  - natural language inference

---
# BERT – Results

For finetuning, dropout 0.1 is used, usually very small number of epochs (2-4) suffice,
and a good learning rate is usually one of 5e-5, 3e-5, 2e-5.

~~~
![w=100%](bert_results.pdf)

---
class: middle
# BERT – Results

![w=33%](bert_results_squad_1.pdf)![w=33%](bert_results_squad_2.pdf)![w=33%](bert_results_swag.pdf)

---
class: middle
# BERT – Ablations

![w=50%](bert_ablations_steps.pdf)![w=50%](bert_ablations_masking.pdf)

---
section: mBERT
# Multilingual BERT

The Multilingual BERT is pre-trained on 102-104 largest Wikipedias, including the
Czech one.

~~~
There are two versions, the _cased_ one has WordPieces including case,
and the _uncased_ one with subwords all in lower case and _without diacritics_.

~~~
Even if only very small percentage of input sentences were Czech, it works
surprisingly well for Czech NLP.

~~~
Furthermore, without any explicit supervision, mBERT is able to represent the
input languages in a _shared_ space, allowing cross-lingual transfer.

---
# Cross-lingual Transfer with Multilingual BERT

![w=50%,f=right](mlqa_data.pdf)

Consider a _reading comprehension_ task, where for a given paragraph and
a question an answer needs to be located in the paragraph.

Then training the model in English and then directly running it on a different
language works comparably to translating the data to English and then back.

![w=90%,h=center](mlqa_results.pdf)

---
section: RoBERTa
# RoBERTa – NSP

The _next sentence prediction_ was originally hypothesized to be an important
factor during training of the BERT model, as indicated by ablation experiments.
However, later experiments indicated removing it might improve results.

~~~
The RoBERTa authors therefore performed the following experiments:

![w=50%,f=right](roberta_nsp.pdf)

- SEGMENT-PAIR: pair of segments with at most 512 tokens in total;
~~~
- SENTENCE-PAIR: pair of _natural sentences_, usually significantly
  shorter than 512 tokens;
~~~
- FULL-SENTENCES: just one segment on input with 512 tokens, can cross document
  boundary;
~~~
- DOC-SENTENCES: just one segment on input with 512 tokens, cannot cross
  document boundary.

---
# RoBERTa – Larger Batches

BERT is trained for 1M steps with a learning rate of 1e-4.

~~~
The RoBERTa authors also considered larger batches (with linearly larger
learning rate).
![w=60%,h=center](roberta_larger_batches.pdf)

---
# RoBERTa

The RoBERTa model, **R**bustly **o**ptimized **BERT** **a**pproach, is trained
with dynamic masking, FULL-SENTENCES without NSP, large 8k minibatches
and byte-level BPE with 50k subwords.

![w=73%,h=center](roberta_larger_data.pdf)

---
# RoBERTa Results

![w=70%,mh=100%,v=middle,f=left](roberta_results.pdf)
![w=29%,mh=50%,v=middle](roberta_results_squad.pdf)
![w=29%,mh=50%,v=bottom](roberta_results_race.pdf)

---
section: ALBERT
# ALBERT

ALBERT model, **A** **L**ite **BERT**, was proposed with small model size in
mind.

~~~
To achieve smaller size, the authors consider
- factorized embeddings representation; and
- parameter sharing across layers.

~~~
The following configurations are evaluated in the paper:
![w=80%,h=center](albert_configurations.pdf)

---
# ALBERT – Factorized Embeddings

The subword embeddings have the hidden size dimensionality $H$ in BERT, which
results in quite a large number of parameters.

~~~
Instead, authors propose to represent the subwords using only embeddings of
size $E$ and then use a matrix of size $E × H$ to generate the correctly-sized
embeddings for the first layer.

![w=80%,h=center](albert_ablations_vocabulary_embedding.pdf)

---
# ALBERT – Shared Parameters

To improve parameter efficiency, the parameters of both the soft-attention and
the feed-forward networks are shared across layers.

![w=80%,h=center,mh=80%,v=middle](albert_ablations_sharing.pdf)

---
# ALBERT – Sentence Order Prediction

An alternative to _next sentence prediction_ is considered – given two
consecutive segments, predict which one appeared first in the original document.

![w=80%,h=center,mh=80%,v=middle](albert_ablations_sop.pdf)

---
# ALBERT Results and Ablations

![w=85%,h=center](albert_results.pdf)

~~~
![w=85%,h=center](albert_ablations_data.pdf)

~~~
![w=85%,h=center](albert_ablations_dropout.pdf)

---
# ALBERT SoTA Results
![w=100%](albert_results_glue.pdf)

---
# ALBERT SoTA Results
![w=100%](albert_results_squad_race.pdf)
